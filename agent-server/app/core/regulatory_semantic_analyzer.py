"""
æ³•è§„è¯­ä¹‰è§£æå™¨ï¼ˆRegulatory Semantic Analystï¼‰

èŒè´£ï¼š
- ä»ç»“æ„å•å…ƒä¸­è¯†åˆ«è§„èŒƒæ€§è§„åˆ™ï¼ˆNormative Rulesï¼‰
- å°†æ— æ³•è§„åˆ™åŒ–çš„å†…å®¹è½¬æ¢ä¸ºè§£é‡Šå‹é—®ç­”ï¼ˆQAï¼‰
- è¿‡æ»¤æ— è¯­ä¹‰ä»·å€¼çš„å†…å®¹ï¼ˆIgnoreï¼‰

æ ¸å¿ƒåŸåˆ™ï¼š
- chunkçš„æœ€å°å•ä½å¿…é¡»æ˜¯"ä¸€æ¡å¯ç‹¬ç«‹åˆ¤æ–­çœŸä¼ªçš„è§„èŒƒæ€§è§„åˆ™"
- è§„åˆ™è¾¹ç•Œç”±å¤§æ¨¡å‹æ ¹æ®æ³•è§„è¯­ä¹‰åˆ¤æ–­ï¼Œè€Œä¸æ˜¯é€šè¿‡æ–‡æœ¬ç»“æ„åˆ¤æ–­
"""

import json
import re
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, asdict
from langchain_core.messages import HumanMessage
from app.core.llm import get_llm
from app.core.config import settings


@dataclass
class StructuralUnit:
    """ç»“æ„å•å…ƒ"""

    content: str  # å•å…ƒå†…å®¹
    unit_type: str  # å•å…ƒç±»å‹ï¼štable_row, sentence/å¥å­, paragraph/æ®µè½, note/æ³¨é‡Š
    page_num: Optional[int] = None  # é¡µç 
    metadata: Optional[Dict[str, Any]] = None  # é¢å¤–å…ƒæ•°æ®


@dataclass
class NormativeRule:
    """è§„èŒƒæ€§è§„åˆ™"""

    document: str  # å®Œæ•´çš„è‡ªç„¶è¯­è¨€æè¿°ï¼ˆä½¿ç”¨æ³•è§„è¯­ä¹‰è¡¨è¾¾ï¼‰
    item: str  # çº¦æŸå¯¹è±¡
    limit_type: str  # çº¦æŸç±»å‹ï¼š>=, <=, range, enum
    limit_value: Union[float, int, str, List]  # æ•°å€¼æˆ–èŒƒå›´
    unit: str  # å•ä½ï¼ˆå¦‚ %ã€mg/kgï¼‰
    condition: str  # é€‚ç”¨æ¡ä»¶ï¼ˆå¦‚"å‘é…µæ³•äº§å“"ï¼‰
    standard_ref: str  # æ ‡å‡†ç¼–å·ï¼ˆå¦‚ GB 28310-2012ï¼‰


@dataclass
class QA:
    """é—®ç­”å¯¹"""

    question: str  # é—®é¢˜
    answer: str  # ç­”æ¡ˆï¼ˆç¨³å®šã€å¯å¤ç”¨ï¼Œä¸åŒ…å«åˆè§„åˆ¤æ–­ç»“è®ºï¼‰
    standard_ref: str  # æ ‡å‡†ç¼–å·


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""

    type: str  # "rule", "qa", "ignore"
    rules: Optional[List[NormativeRule]] = None
    qas: Optional[List[QA]] = None


class RegulatorySemanticAnalyzer:
    """æ³•è§„è¯­ä¹‰è§£æå™¨"""

    def __init__(self, standard_ref: Optional[str] = None):
        """
        åˆå§‹åŒ–è§£æå™¨

        Args:
            standard_ref: æ ‡å‡†ç¼–å·ï¼ˆå¦‚ "GB 28310-2012"ï¼‰
        """
        self.standard_ref = standard_ref or "æœªçŸ¥æ ‡å‡†"
        self.llm = get_llm()

    def analyze_unit(self, unit: StructuralUnit) -> AnalysisResult:
        """
        åˆ†æå•ä¸ªç»“æ„å•å…ƒ

        Args:
            unit: ç»“æ„å•å…ƒ

        Returns:
            AnalysisResult: åˆ†æç»“æœ
        """
        prompt = self._build_analysis_prompt(unit)

        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])

            if hasattr(response, "content"):
                response_text = response.content
            elif isinstance(response, str):
                response_text = response
            else:
                response_text = str(response)

            # è§£æå¤§æ¨¡å‹è¿”å›çš„ç»“æœ
            result = self._parse_llm_response(response_text, unit)
            return result

        except Exception as e:
            print(f"è§£æç»“æ„å•å…ƒå¤±è´¥ (ç±»å‹: {unit.unit_type}): {e}")
            # è¿”å›ignoreç»“æœ
            return AnalysisResult(type="ignore")

    def _build_analysis_prompt(self, unit: StructuralUnit) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€åæ³•è§„è¯­ä¹‰è§£æä¸“å®¶ï¼ˆRegulatory Semantic Analystï¼‰ï¼Œ
æ“…é•¿ä»å›½å®¶é£Ÿå“å®‰å…¨æ ‡å‡†ã€è¡Œä¸šæ ‡å‡†ä¸­ï¼Œè¯†åˆ«å¯ç”¨äºåˆè§„åˆ¤æ–­çš„è§„èŒƒæ€§è§„åˆ™ï¼ˆNormative Rulesï¼‰ï¼Œå¹¶å°†æ— æ³•è§„åˆ™åŒ–çš„å†…å®¹è½¬æ¢ä¸ºè§£é‡Šå‹é—®ç­”ï¼ˆQAï¼‰ã€‚

## è¾“å…¥è¯´æ˜

ä½ å°†æ”¶åˆ°ä¸€ä¸ª"ç»“æ„å•å…ƒï¼ˆStructural Unitï¼‰"ï¼Œè¯¥å•å…ƒå¯èƒ½æ¥æºäºï¼š
- è¡¨æ ¼ä¸­çš„ä¸€è¡Œ
- ç« èŠ‚ä¸­çš„ä¸€å¥æˆ–ä¸€æ®µ
- æ³¨é‡Šï¼ˆå¦‚"æ³¨ï¼šâ€¦â€¦"ï¼‰

è¯¥ç»“æ„å•å…ƒä¸ä¸€å®šå®Œæ•´ï¼Œä¹Ÿä¸ä¸€å®šæ˜¯è§„åˆ™ã€‚

## é‡è¦è®¾è®¡åŸåˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰

- ä¸è¦ç›´æ¥å‘é‡åŒ– PDF åŸæ–‡ã€OCR æ–‡æœ¬æˆ– Markdown å†…å®¹
- ä¸è¦ä½¿ç”¨ä¼ ç»Ÿ chunk ç­–ç•¥ï¼ˆæŒ‰å­—æ•°ã€æ®µè½ã€æ ‡é¢˜ã€çˆ¶å­ chunkï¼‰
- chunk çš„æœ€å°å•ä½å¿…é¡»æ˜¯"ä¸€æ¡å¯ç‹¬ç«‹åˆ¤æ–­çœŸä¼ªçš„è§„èŒƒæ€§è§„åˆ™ï¼ˆNormative Ruleï¼‰"
- è§„åˆ™çš„è¾¹ç•Œå¿…é¡»ç”±å¤§æ¨¡å‹æ ¹æ®æ³•è§„è¯­ä¹‰åˆ¤æ–­ï¼Œè€Œä¸æ˜¯é€šè¿‡æ–‡æœ¬ç»“æ„åˆ¤æ–­

## ä½ çš„ä»»åŠ¡ï¼ˆä¸¥æ ¼æŒ‰é¡ºåºï¼‰

### Step 1ï¼šåˆ¤æ–­è¯¥ç»“æ„å•å…ƒçš„ç±»å‹ï¼ˆå¿…é¡»ï¼‰

ä½ å¿…é¡»å…ˆåˆ¤æ–­è¯¥ç»“æ„å•å…ƒå±äºä»¥ä¸‹å“ªä¸€ç±»ï¼ˆåªèƒ½é€‰ä¸€ç±»ï¼‰ï¼š

- "rule"ï¼šå¯æ„æˆä¸€æ¡æˆ–å¤šæ¡è§„èŒƒæ€§è§„åˆ™
- "qa"ï¼šä¸æ„æˆè§„åˆ™ï¼Œä½†å…·æœ‰è§£é‡Šã€å®šä¹‰æˆ–èƒŒæ™¯ä»·å€¼
- "ignore"ï¼šçº¯å¼•ç”¨ã€çº¯æŒ‡å¼•ã€æ— è¯­ä¹‰ä»·å€¼ï¼ˆå¦‚"è§è¡¨2""è§é™„å½•A"ï¼‰

### Step 2Aï¼šå¦‚æœç±»å‹ä¸º "rule"

ä½ å¿…é¡»æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

1ï¸âƒ£ è§„åˆ™è¾¹ç•Œåˆ¤æ–­

åˆ¤æ–­è¯¥ç»“æ„å•å…ƒåŒ…å«ï¼š
- 0 æ¡
- 1 æ¡
- å¤šæ¡

è§„èŒƒæ€§è§„åˆ™

âš ï¸ ä¸å¾—å°†å¤šæ¡è§„åˆ™åˆå¹¶ä¸ºä¸€æ¡ã€‚

2ï¸âƒ£ è§„åˆ™é‡å†™ï¼ˆå¿…é¡»ï¼‰

å¯¹æ¯ä¸€æ¡è§„åˆ™ï¼š
- ä½¿ç”¨å®Œæ•´ã€æ˜ç¡®çš„è‡ªç„¶è¯­è¨€
- ä½¿ç”¨æ³•è§„è¯­ä¹‰è¡¨è¾¾ï¼š
  - "ä¸å¾—ä½äº"
  - "ä¸å¾—è¶…è¿‡"
  - "åº”å½“"
- ä¸å¾—ä½¿ç”¨ â‰¥ â‰¤ ç­‰ç¬¦å·
- ä¸å¾—å¼•ç”¨"è§ä¸Šæ–‡ / è¡¨X"

3ï¸âƒ£ è§„åˆ™ç»“æ„åŒ–æŠ½å–ï¼ˆå¿…é¡»ï¼‰

å¯¹æ¯æ¡è§„åˆ™ï¼Œæå–ä»¥ä¸‹å­—æ®µï¼š
- itemï¼šçº¦æŸå¯¹è±¡
- limit_typeï¼šçº¦æŸç±»å‹ï¼ˆ>=, <=, range, enumï¼‰
- limit_valueï¼šæ•°å€¼æˆ–èŒƒå›´
- unitï¼šå•ä½ï¼ˆå¦‚ %ã€mg/kgï¼‰
- conditionï¼šé€‚ç”¨æ¡ä»¶ï¼ˆå¦‚"å‘é…µæ³•äº§å“"ï¼‰
- standard_refï¼šæ ‡å‡†ç¼–å·ï¼ˆå¦‚ GB 28310-2012ï¼‰

### Step 2Bï¼šå¦‚æœç±»å‹ä¸º "qa"

ä½ å¿…é¡»æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

1ï¸âƒ£ é—®é¢˜ç”Ÿæˆ

æç‚¼ 1 ä¸ªæˆ–å¤šä¸ªæ˜ç¡®é—®é¢˜
- é—®é¢˜åº”ç¬¦åˆçœŸå®ç”¨æˆ·å¯èƒ½æå‡ºçš„é—®é¢˜
- ä¸å¾—æ˜¯"æœ¬æ ‡å‡†æ˜¯ä»€ä¹ˆ"è¿™ç±»ç©ºæ³›é—®é¢˜

2ï¸âƒ£ ç­”æ¡ˆç”Ÿæˆ

ç­”æ¡ˆå¿…é¡»ï¼š
- ç¨³å®š
- å¯å¤ç”¨
- ä¸åŒ…å«åˆè§„åˆ¤æ–­ç»“è®º
- ä¸åŒ…å«é˜ˆå€¼åˆ¤å®šç»“æœ
- ç­”æ¡ˆç”¨äºè§£é‡Šè§„åˆ™ï¼Œè€Œä¸æ˜¯æ›¿ä»£è§„åˆ™

### Step 2Cï¼šå¦‚æœç±»å‹ä¸º "ignore"

è¾“å‡ºç©ºæ•°ç»„
ä¸ç”Ÿæˆ Rule æˆ– QA

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰

å¦‚æœæ˜¯ Rule:
{{
  "type": "rule",
  "rules": [
    {{
      "document": "Natural language description of the rule.",
      "item": "...",
      "limit_type": "...",
      "limit_value": ...,
      "unit": "...",
      "condition": "...",
      "standard_ref": "{self.standard_ref}"
    }}
  ]
}}

å¦‚æœæ˜¯ QA:
{{
  "type": "qa",
  "qas": [
    {{
      "question": "...",
      "answer": "...",
      "standard_ref": "{self.standard_ref}"
    }}
  ]
}}

å¦‚æœæ˜¯ Ignore:
{{
  "type": "ignore"
}}

## âš ï¸ ä¸¥æ ¼çº¦æŸï¼ˆè¿åå³é”™è¯¯ï¼‰

âŒ ä¸å¾—åŒæ—¶è¾“å‡º rule å’Œ qa
âŒ ä¸å¾—è¾“å‡ºä¸å®Œæ•´è§„åˆ™
âŒ ä¸å¾—ä½¿ç”¨ Markdown
âŒ ä¸å¾—å¼•ç”¨"æœ¬è¡¨""ä¸Šæ–‡"
âŒ ä¸å¾—è¾“å‡ºåŸæ–‡ç…§æŠ„

## ğŸ§  å†…éƒ¨åˆ¤æ–­åŸåˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰

- å¦‚æœä¸€å¥è¯ å¯ä»¥å†™æˆ if / else â†’ Rule
- å¦‚æœä¸€å¥è¯ åªèƒ½è§£é‡Š why / what â†’ QA
- å¦‚æœä¸€å¥è¯ æ—¢ä¸èƒ½åˆ¤æ–­ï¼Œä¹Ÿä¸èƒ½è§£é‡Š â†’ Ignore

## ç»“æ„å•å…ƒä¿¡æ¯

æ ‡å‡†ç¼–å·: {self.standard_ref}
ç»“æ„å•å…ƒç±»å‹: {unit.unit_type}
{f"æ³¨æ„ï¼šä»¥ä¸‹å†…å®¹åŒ…å«å¤šä¸ª{unit.unit_type}å•å…ƒåˆå¹¶åçš„å†…å®¹ï¼Œè¯·åˆ†åˆ«åˆ†ææ¯ä¸ªå•å…ƒã€‚" if unit.metadata and unit.metadata.get("batch_size", 0) > 1 else ""}
ç»“æ„å•å…ƒå†…å®¹:
{unit.content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚è¿›è¡Œåˆ†æï¼Œåªè¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜ã€‚"""

        return prompt

    def _parse_llm_response(
        self, response_text: str, unit: StructuralUnit
    ) -> AnalysisResult:
        """è§£æå¤§æ¨¡å‹è¿”å›çš„ç»“æœ"""
        # å°è¯•æå–JSON
        json_match = re.search(
            r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}", response_text, re.DOTALL
        )

        if not json_match:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•æŸ¥æ‰¾typeå­—æ®µ
            if '"type"' in response_text or "'type'" in response_text:
                # å°è¯•æå–æ•´ä¸ªJSONå¯¹è±¡
                json_match = re.search(r'\{.*"type".*\}', response_text, re.DOTALL)

        if json_match:
            try:
                json_str = json_match.group(0)
                # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                json_str = re.sub(r"```json\s*", "", json_str)
                json_str = re.sub(r"```\s*", "", json_str)
                json_str = json_str.strip()

                data = json.loads(json_str)
                result_type = data.get("type", "ignore")

                if result_type == "rule":
                    rules = []
                    rules_data = data.get("rules", [])
                    for rule_data in rules_data:
                        rule = NormativeRule(
                            document=rule_data.get("document", ""),
                            item=rule_data.get("item", ""),
                            limit_type=rule_data.get("limit_type", ""),
                            limit_value=rule_data.get("limit_value", ""),
                            unit=rule_data.get("unit", ""),
                            condition=rule_data.get("condition", ""),
                            standard_ref=rule_data.get(
                                "standard_ref", self.standard_ref
                            ),
                        )
                        # éªŒè¯è§„åˆ™å®Œæ•´æ€§
                        if rule.document and rule.item:
                            rules.append(rule)

                    return AnalysisResult(type="rule", rules=rules)

                elif result_type == "qa":
                    qas = []
                    qas_data = data.get("qas", [])
                    for qa_data in qas_data:
                        qa = QA(
                            question=qa_data.get("question", ""),
                            answer=qa_data.get("answer", ""),
                            standard_ref=qa_data.get("standard_ref", self.standard_ref),
                        )
                        # éªŒè¯QAå®Œæ•´æ€§
                        if qa.question and qa.answer:
                            qas.append(qa)

                    return AnalysisResult(type="qa", qas=qas)

                else:
                    return AnalysisResult(type="ignore")

            except json.JSONDecodeError as e:
                print(f"JSONè§£æå¤±è´¥: {e}")
                print(f"å“åº”æ–‡æœ¬ç‰‡æ®µ: {response_text[:500]}")
                return AnalysisResult(type="ignore")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSONï¼Œè¿”å›ignore
        return AnalysisResult(type="ignore")

    def analyze_units(self, units: List[StructuralUnit]) -> Dict[str, Any]:
        """
        æ‰¹é‡åˆ†æç»“æ„å•å…ƒï¼ˆä¼˜åŒ–ç‰ˆï¼šæŒ‰ç±»å‹åˆ†ç»„æ‰¹é‡å¤„ç†ï¼‰

        Args:
            units: ç»“æ„å•å…ƒåˆ—è¡¨

        Returns:
            åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸ï¼š
            {
                "rules": List[NormativeRule],
                "qas": List[QA],
                "ignored_count": int
            }
        """
        all_rules = []
        all_qas = []
        ignored_count = 0

        # æŒ‰unit_typeåˆ†ç»„
        units_by_type: Dict[str, List[StructuralUnit]] = {}
        for unit in units:
            unit_type = unit.unit_type
            if unit_type not in units_by_type:
                units_by_type[unit_type] = []
            units_by_type[unit_type].append(unit)

        print(f"\nç»“æ„å•å…ƒåˆ†ç»„ç»Ÿè®¡: {[(k, len(v)) for k, v in units_by_type.items()]}")

        # å¤„ç†æ¯ç§ç±»å‹çš„å•å…ƒ
        for unit_type, type_units in units_by_type.items():
            print(f"\nå¤„ç† {unit_type} ç±»å‹å•å…ƒï¼ˆå…± {len(type_units)} ä¸ªï¼‰...")

            if unit_type == "sentence" or unit_type == "note":
                # sentenceå’Œnoteï¼šåˆå¹¶åæ‰¹é‡å¤„ç†
                result = self._analyze_sentences_batch(type_units)
            elif unit_type == "table_row":
                # table_rowï¼šæ‰¹é‡å¤„ç†è¡¨æ ¼è¡Œ
                result = self._analyze_table_rows_batch(type_units)
            elif unit_type == "paragraph":
                # paragraphï¼šTODO åç»­å¤„ç†
                print(f"  âš  paragraphç±»å‹æš‚æœªå®ç°æ‰¹é‡å¤„ç†ï¼Œé€ä¸ªå¤„ç†...")
                result = self._analyze_units_one_by_one(type_units)
            else:
                # å…¶ä»–ç±»å‹ï¼šé€ä¸ªå¤„ç†
                result = self._analyze_units_one_by_one(type_units)

            # æ±‡æ€»ç»“æœ
            if result["rules"]:
                all_rules.extend(result["rules"])
            if result["qas"]:
                all_qas.extend(result["qas"])
            ignored_count += result["ignored_count"]

        return {
            "rules": all_rules,
            "qas": all_qas,
            "ignored_count": ignored_count,
            "total_units": len(units),
        }

    def _analyze_sentences_batch(self, units: List[StructuralUnit]) -> Dict[str, Any]:
        """
        æ‰¹é‡åˆ†æsentenceå’Œnoteç±»å‹çš„å•å…ƒ

        ç®€å•å¤„ç†ï¼šå°†æ¯ä¸ªunité‡æ–°ç»„ç»‡æˆä¸€å¥ç¬¦åˆé€»è¾‘çš„è¯ï¼Œä¸éœ€è¦rule/qaåˆ†ç±»
        """
        if not units:
            return {"rules": [], "qas": [], "ignored_count": 0}

        print(f"  æ‰¹é‡å¤„ç† {len(units)} ä¸ª{units[0].unit_type}å•å…ƒï¼ˆç®€åŒ–æ¨¡å¼ï¼‰...")

        # æ„å»ºæ‰¹é‡å¤„ç†çš„prompt
        prompt = self._build_sentence_rewrite_prompt(units)

        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])

            if hasattr(response, "content"):
                response_text = response.content
            elif isinstance(response, str):
                response_text = response
            else:
                response_text = str(response)

            # è§£æLLMè¿”å›çš„é‡å†™ç»“æœ
            rewritten_sentences = self._parse_sentence_rewrite_response(
                response_text, len(units)
            )

            # å°†é‡å†™åçš„å¥å­è½¬æ¢ä¸ºDocumentæ ¼å¼ï¼ˆä½œä¸ºQAå­˜å‚¨ï¼‰
            qas = []
            for i, rewritten in enumerate(rewritten_sentences):
                if rewritten and len(rewritten.strip()) > 5:  # è¿‡æ»¤ç©ºç»“æœ
                    original = units[i].content if i < len(units) else ""
                    # åˆ›å»ºQAå¯¹ï¼šé—®é¢˜å¯ä»¥æ˜¯åŸå¥ï¼Œç­”æ¡ˆæ˜¯é‡å†™åçš„å¥å­
                    qa = QA(
                        question=f"è¯·è§£é‡Šï¼š{original[:100]}",
                        answer=rewritten,
                        standard_ref=self.standard_ref,
                    )
                    qas.append(qa)

            return {
                "rules": [],
                "qas": qas,
                "ignored_count": len(units) - len(qas),  # æœªæˆåŠŸé‡å†™çš„ç®—ä½œå¿½ç•¥
            }

        except Exception as e:
            print(f"  æ‰¹é‡å¤„ç†sentence/noteå¤±è´¥: {e}")
            return {"rules": [], "qas": [], "ignored_count": len(units)}

    def _build_sentence_rewrite_prompt(self, units: List[StructuralUnit]) -> str:
        """æ„å»ºsentence/noteæ‰¹é‡é‡å†™çš„prompt"""
        unit_type = units[0].unit_type if units else "sentence"
        unit_type_name = "å¥å­" if unit_type == "sentence" else "æ³¨é‡Š"

        # æ„å»ºè¾“å…¥å†…å®¹åˆ—è¡¨
        content_list = []
        for i, unit in enumerate(units, 1):
            content_list.append(f"{i}. {unit.content}")

        content_text = "\n".join(content_list)

        prompt = f"""ä½ æ˜¯ä¸€åæ–‡æœ¬æ•´ç†ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹{unit_type_name}é‡æ–°ç»„ç»‡æˆç¬¦åˆé€»è¾‘ã€è¡¨è¾¾æ¸…æ™°çš„å¥å­ã€‚

è¦æ±‚ï¼š
1. ä¿æŒåŸæ„ä¸å˜
2. ä½¿ç”¨è§„èŒƒçš„æ³•è§„è¯­è¨€è¡¨è¾¾
3. å¥å­å®Œæ•´ã€é€šé¡º
4. å¦‚æœåŸå¥å·²ç»æ˜¯è§„èŒƒè¡¨è¾¾ï¼Œå¯ä»¥ä¿æŒåŸæ ·

è¾“å…¥{unit_type_name}åˆ—è¡¨ï¼š
{content_text}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ¯ä¸ªè¾“å…¥å¯¹åº”ä¸€ä¸ªè¾“å‡ºï¼š
{{
  "sentences": [
    "é‡å†™åçš„ç¬¬1ä¸ªå¥å­",
    "é‡å†™åçš„ç¬¬2ä¸ªå¥å­",
    ...
  ]
}}

æ³¨æ„ï¼š
- è¾“å‡ºæ•°ç»„çš„é•¿åº¦å¿…é¡»ä¸è¾“å…¥æ•°é‡ç›¸åŒ
- å¦‚æœæŸä¸ªè¾“å…¥æ— æ³•é‡å†™æˆ–æ²¡æœ‰æ„ä¹‰ï¼Œè¾“å‡ºç©ºå­—ç¬¦ä¸²""
- åªè¿”å›JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Š"""

        return prompt

    def _parse_sentence_rewrite_response(
        self, response_text: str, expected_count: int
    ) -> List[str]:
        """è§£æsentenceé‡å†™çš„å“åº”"""
        sentences = []

        # å°è¯•æå–JSON
        json_match = re.search(
            r'\{[^{}]*"sentences"[^{}]*\[.*?\]\s*\}', response_text, re.DOTALL
        )

        if json_match:
            try:
                json_str = json_match.group(0)
                # æ¸…ç†markdownä»£ç å—æ ‡è®°
                json_str = re.sub(r"```json\s*", "", json_str)
                json_str = re.sub(r"```\s*", "", json_str)
                json_str = json_str.strip()

                data = json.loads(json_str)
                sentences = data.get("sentences", [])

                # ç¡®ä¿æ•°é‡åŒ¹é…
                if len(sentences) < expected_count:
                    # å¦‚æœæ•°é‡ä¸è¶³ï¼Œç”¨ç©ºå­—ç¬¦ä¸²è¡¥é½
                    sentences.extend([""] * (expected_count - len(sentences)))
                elif len(sentences) > expected_count:
                    # å¦‚æœæ•°é‡è¿‡å¤šï¼Œæˆªæ–­
                    sentences = sentences[:expected_count]

            except json.JSONDecodeError as e:
                print(f"  è§£æsentenceé‡å†™ç»“æœå¤±è´¥: {e}")
                sentences = [""] * expected_count
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œå°è¯•æŒ‰è¡Œåˆ†å‰²
            lines = [line.strip() for line in response_text.split("\n") if line.strip()]
            sentences = lines[:expected_count]
            if len(sentences) < expected_count:
                sentences.extend([""] * (expected_count - len(sentences)))

        return sentences

    def _analyze_table_rows_batch(self, units: List[StructuralUnit]) -> Dict[str, Any]:
        """
        æ‰¹é‡åˆ†ætable_rowç±»å‹çš„å•å…ƒ

        è¡¨æ ¼è¡Œå¯ä»¥æŒ‰è¡¨æ ¼åˆ†ç»„ï¼ŒåŒä¸€è¡¨æ ¼çš„è¡Œä¸€èµ·å¤„ç†
        """
        if not units:
            return {"rules": [], "qas": [], "ignored_count": 0}

        all_rules = []
        all_qas = []
        ignored_count = 0

        # æŒ‰è¡¨æ ¼åˆ†ç»„ï¼ˆé€šè¿‡metadataä¸­çš„table_indexï¼‰
        tables: Dict[int, List[StructuralUnit]] = {}
        for unit in units:
            table_idx = unit.metadata.get("table_index") if unit.metadata else None
            if table_idx is None:
                # å¦‚æœæ²¡æœ‰table_indexï¼Œæ¯ä¸ªå•å…ƒå•ç‹¬å¤„ç†
                result = self.analyze_unit(unit)
                if result.rules:
                    all_rules.extend(result.rules)
                if result.qas:
                    all_qas.extend(result.qas)
                if result.type == "ignore":
                    ignored_count += 1
            else:
                if table_idx not in tables:
                    tables[table_idx] = []
                tables[table_idx].append(unit)

        # æ‰¹é‡å¤„ç†æ¯ä¸ªè¡¨æ ¼
        for table_idx, table_rows in tables.items():
            if len(table_rows) == 1:
                # å•ä¸ªè¡Œï¼Œç›´æ¥å¤„ç†
                result = self.analyze_unit(table_rows[0])
            else:
                # å¤šä¸ªè¡Œï¼Œåˆå¹¶å¤„ç†
                print(f"  æ‰¹é‡å¤„ç†è¡¨æ ¼ {table_idx}ï¼ˆå…± {len(table_rows)} è¡Œï¼‰...")

                # æ„å»ºè¡¨æ ¼å†…å®¹
                table_content = "\n".join([row.content for row in table_rows])

                # è·å–è¡¨å¤´ï¼ˆä»ç¬¬ä¸€è¡Œçš„metadataï¼‰
                header = (
                    table_rows[0].metadata.get("header", [])
                    if table_rows[0].metadata
                    else []
                )

                # åˆ›å»ºåˆå¹¶åçš„å•å…ƒ
                combined_unit = StructuralUnit(
                    content=table_content,
                    unit_type="table_row",
                    page_num=table_rows[0].page_num if table_rows else None,
                    metadata={
                        "table_index": table_idx,
                        "header": header,
                        "row_count": len(table_rows),
                        "batch_processing": True,
                    },
                )

                result = self.analyze_unit(combined_unit)

            if result.rules:
                all_rules.extend(result.rules)
            if result.qas:
                all_qas.extend(result.qas)
            if result.type == "ignore":
                ignored_count += 1

        return {"rules": all_rules, "qas": all_qas, "ignored_count": ignored_count}

    def _analyze_units_one_by_one(self, units: List[StructuralUnit]) -> Dict[str, Any]:
        """
        é€ä¸ªåˆ†æå•å…ƒï¼ˆç”¨äºparagraphç­‰æš‚æœªä¼˜åŒ–çš„ç±»å‹ï¼‰
        """
        all_rules = []
        all_qas = []
        ignored_count = 0

        for i, unit in enumerate(units, 1):
            print(f"  å¤„ç†å•å…ƒ {i}/{len(units)}: {unit.unit_type}")
            result = self.analyze_unit(unit)

            if result.rules:
                all_rules.extend(result.rules)
            if result.qas:
                all_qas.extend(result.qas)
            if result.type == "ignore":
                ignored_count += 1

        return {"rules": all_rules, "qas": all_qas, "ignored_count": ignored_count}

    def to_dict(self, result: AnalysisResult) -> Dict[str, Any]:
        """å°†åˆ†æç»“æœè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆç”¨äºJSONåºåˆ—åŒ–ï¼‰"""
        if result.type == "rule" and result.rules:
            return {
                "type": "rule",
                "rules": [
                    {
                        "document": rule.document,
                        "item": rule.item,
                        "limit_type": rule.limit_type,
                        "limit_value": rule.limit_value,
                        "unit": rule.unit,
                        "condition": rule.condition,
                        "standard_ref": rule.standard_ref,
                    }
                    for rule in result.rules
                ],
            }
        elif result.type == "qa" and result.qas:
            return {
                "type": "qa",
                "qas": [
                    {
                        "question": qa.question,
                        "answer": qa.answer,
                        "standard_ref": qa.standard_ref,
                    }
                    for qa in result.qas
                ],
            }
        else:
            return {"type": "ignore"}


# å…¨å±€è§£æå™¨å®ä¾‹ï¼ˆéœ€è¦åœ¨ä½¿ç”¨æ—¶æŒ‡å®šstandard_refï¼‰
# regulatory_analyzer = RegulatorySemanticAnalyzer()
