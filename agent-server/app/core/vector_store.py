"""
å‘é‡å­˜å‚¨ç®¡ç† - åŸºäºChromaDB (LangChainç‰ˆæœ¬)
"""

import os
import uuid
import re
import time
from typing import List, Optional, Dict, Any
from langchain_chroma import Chroma
from langchain_core.documents import Document
from app.core.config import settings
from app.core.embeddings import get_embedding_model
from app.core.parent_store import ParentChunkStore
from app.core.document_loader import document_loader


class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""

    # ==================== åˆå§‹åŒ–æ–¹æ³• ====================

    def __init__(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        os.makedirs(settings.CHROMA_PERSIST_DIR, exist_ok=True)

        # è·å–åµŒå…¥æ¨¡å‹
        self.embedding_model = get_embedding_model(
            provider_name=settings.EMBEDDING_PROVIDER,
        )

        # åˆå§‹åŒ–ChromaDBå‘é‡å­˜å‚¨
        self.vector_store = Chroma(
            persist_directory=settings.CHROMA_PERSIST_DIR,
            collection_name=settings.CHROMA_COLLECTION_NAME,
            embedding_function=self.embedding_model,
        )

        # çˆ¶ chunk å­˜å‚¨ï¼ˆSQLiteï¼‰
        self.parent_store = ParentChunkStore(
            db_path=os.path.join(settings.CHROMA_PERSIST_DIR, "parent_chunks.sqlite3")
        )

    # ==================== å†…éƒ¨å‡½æ•°ï¼ˆç§æœ‰æ–¹æ³•ï¼‰ ====================

    def _clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬ï¼šæ›¿æ¢è¿ç»­çš„ç©ºæ ¼ã€æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦
        - è¿ç»­çš„ç©ºæ ¼ -> å•ä¸ªç©ºæ ¼
        - è¿ç»­çš„æ¢è¡Œç¬¦ -> å•ä¸ªæ¢è¡Œç¬¦
        - è¿ç»­çš„åˆ¶è¡¨ç¬¦ -> å•ä¸ªç©ºæ ¼
        """
        # æ›¿æ¢è¿ç»­çš„åˆ¶è¡¨ç¬¦ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'\t+', ' ', text)
        # æ›¿æ¢è¿ç»­çš„ç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r' +', ' ', text)
        # æ›¿æ¢è¿ç»­çš„æ¢è¡Œç¬¦ä¸ºå•ä¸ªæ¢è¡Œç¬¦ï¼ˆä¿ç•™æ¢è¡Œç¬¦ï¼Œå› ä¸ºç”¨äºåˆ‡åˆ†ï¼‰
        text = re.sub(r'\n+', '\n', text)
        # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
        return text.strip()

    def _split_parent_chunks(self, text: str) -> List[str]:
        """
        æŒ‰ç…§difyè®¾è®¡åˆ‡åˆ†çˆ¶å±‚çº§chunk
        - æŒ‰ç…§æ®µè½ï¼ˆPARENT_SEPARATORï¼Œé»˜è®¤\n\nï¼‰åˆ†å‰²
        - æ¯ä¸ªchunkæœ€å¤§é•¿åº¦ä¸ºPARENT_CHUNK_SIZEï¼ˆé»˜è®¤1024ï¼‰
        """
        # å…ˆæŒ‰ç…§çˆ¶å±‚çº§åˆ†éš”ç¬¦åˆ†å‰²æ®µè½ï¼ˆåœ¨æ¸…ç†ä¹‹å‰ï¼Œä¿ç•™åˆ†éš”ç¬¦ï¼‰
        paragraphs = text.split(settings.PARENT_SEPARATOR)
        
        parent_chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # æ¸…ç†æ®µè½æ–‡æœ¬
            paragraph = self._clean_text(paragraph)
            if not paragraph:
                continue
            
            # å¦‚æœå½“å‰chunkåŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ™åˆå¹¶
            if not current_chunk:
                current_chunk = paragraph
            elif len(current_chunk) + len(settings.PARENT_SEPARATOR) + len(paragraph) <= settings.PARENT_CHUNK_SIZE:
                current_chunk += settings.PARENT_SEPARATOR + paragraph
            else:
                # ä¿å­˜å½“å‰chunkï¼Œå¼€å§‹æ–°chunk
                if current_chunk:
                    parent_chunks.append(current_chunk)
                current_chunk = paragraph
                
                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œéœ€è¦å¼ºåˆ¶åˆ†å‰²
                if len(paragraph) > settings.PARENT_CHUNK_SIZE:
                    # æŒ‰å­—ç¬¦å¼ºåˆ¶åˆ†å‰²
                    while len(paragraph) > settings.PARENT_CHUNK_SIZE:
                        parent_chunks.append(paragraph[:settings.PARENT_CHUNK_SIZE])
                        paragraph = paragraph[settings.PARENT_CHUNK_SIZE:]
                    current_chunk = paragraph
        
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk:
            parent_chunks.append(current_chunk)
        
        return parent_chunks

    def _add_documents_parent_child(self, raw_documents: List[Document]) -> None:
        """
        çˆ¶å­ chunk å…¥åº“ç­–ç•¥ï¼ˆDifyé£æ ¼ï¼‰ï¼š
        - parent: ä»…ä¿å­˜åˆ° SQLiteï¼ˆé¿å…å‘é‡åº“é‡å¤å­˜å¤§æ®µæ–‡æœ¬ï¼‰
        - child: ä¿å­˜åˆ° Chromaï¼ˆç”¨äºå‘é‡æ£€ç´¢ï¼‰
        - æŒ‰ç…§æ®µè½ï¼ˆ\n\nï¼‰åˆ‡åˆ†çˆ¶å±‚çº§ï¼Œæœ€å¤§é•¿åº¦1024
        - æŒ‰ç…§è¡Œï¼ˆ\nï¼‰åˆ‡åˆ†å­å—ï¼Œæœ€å¤§é•¿åº¦512
        - æ¸…ç†è¿ç»­çš„ç©ºæ ¼ã€æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦
        """
        child_docs_to_add: List[Document] = []
        child_ids: List[str] = []

        # å¤„ç†æ¯ä¸ªåŸå§‹æ–‡æ¡£
        for doc in raw_documents:
            text = doc.page_content or ""
            
            # åˆ‡åˆ†çˆ¶chunk
            parent_texts = self._split_parent_chunks(text)
            
            for parent_idx, parent_text in enumerate(parent_texts):
                parent_id = str(uuid.uuid4())

                parent_metadata: Dict[str, Any] = dict(doc.metadata or {})
                parent_metadata.update(
                    {
                        "chunk_type": "parent",
                        "split_strategy": "parent_child",
                        "parent_id": parent_id,
                        "parent_index": parent_idx,
                    }
                )

                # å†™çˆ¶ chunk åˆ° SQLite
                self.parent_store.upsert_parent(
                    parent_id=parent_id,
                    text=parent_text,
                    metadata=parent_metadata,
                )

                # ä¸ºçˆ¶ chunk åˆ›å»ºå­ chunkï¼ˆåªå°†å­ chunk å†™å…¥å‘é‡åº“ï¼‰
                child_docs = document_loader.split_child_chunks_for_parent(
                    parent_text, doc.metadata, parent_idx
                )
                for child_idx, child_doc in enumerate(child_docs):
                    child_metadata: Dict[str, Any] = dict(child_doc.metadata or {})
                    child_metadata.update(
                        {
                            "chunk_type": "child",
                            "split_strategy": document_loader.split_strategy,
                            "parent_id": parent_id,
                            "parent_index": parent_idx,
                            "child_index": child_idx,
                        }
                    )
                    child_doc.metadata = child_metadata

                    child_id = f"{parent_id}:{child_idx}"
                    child_ids.append(child_id)
                    child_docs_to_add.append(child_doc)

        if child_docs_to_add:
            # æ‰¹é‡æ·»åŠ å­chunkï¼Œæ·»åŠ é‡è¯•æœºåˆ¶å’Œåˆ†æ‰¹å¤„ç†
            self._add_documents_with_retry(child_docs_to_add, child_ids)
    
    def _add_documents_with_retry(
        self, 
        documents: List[Document], 
        ids: List[str],
        batch_size: int = 50,
        max_retries: int = 3,
        retry_delay: float = 2.0
    ):
        """
        å¸¦é‡è¯•æœºåˆ¶å’Œåˆ†æ‰¹å¤„ç†çš„æ–‡æ¡£æ·»åŠ æ–¹æ³•
        
        Args:
            documents: è¦æ·»åŠ çš„æ–‡æ¡£åˆ—è¡¨
            ids: æ–‡æ¡£IDåˆ—è¡¨
            batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        total = len(documents)
        if total == 0:
            return
        
        print(f"  å¼€å§‹æ‰¹é‡æ·»åŠ  {total} ä¸ªæ–‡æ¡£ï¼ˆæ¯æ‰¹ {batch_size} ä¸ªï¼‰...")
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, total, batch_size):
            batch_end = min(batch_start + batch_size, total)
            batch_docs = documents[batch_start:batch_end]
            batch_ids = ids[batch_start:batch_end]
            
            # é‡è¯•æœºåˆ¶
            for attempt in range(max_retries):
                try:
                    self.vector_store.add_documents(batch_docs, ids=batch_ids)
                    print(f"  âœ“ æˆåŠŸæ·»åŠ æ‰¹æ¬¡ {batch_start//batch_size + 1} ({batch_start+1}-{batch_end}/{total})")
                    break
                except Exception as e:
                    error_msg = str(e)
                    is_connection_error = any(keyword in error_msg.lower() for keyword in [
                        'disconnected', 'connection', 'timeout', 'remote', 'protocol'
                    ])
                    
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (attempt + 1)
                        print(f"  âš  æ‰¹æ¬¡ {batch_start//batch_size + 1} æ·»åŠ å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg[:100]}")
                        print(f"  â³ {wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        print(f"  âœ— æ‰¹æ¬¡ {batch_start//batch_size + 1} æ·»åŠ å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {error_msg}")
                        # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥OllamaæœåŠ¡
                        if is_connection_error:
                            print(f"  ğŸ’¡ æç¤º: å¯èƒ½æ˜¯OllamaæœåŠ¡è¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥:")
                            print(f"     - OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ: curl {settings.OLLAMA_BASE_URL}/api/tags")
                            print(f"     - ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                            print(f"     - æ˜¯å¦å¤„ç†äº†å¤ªå¤šæ–‡æ¡£ï¼Œå¯ä»¥å‡å°batch_size")
                        raise

    # ==================== å…¬å¼€å‡½æ•°ï¼ˆæ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼‰ ====================

    def add_documents(self, documents: List[Document]):
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
        
        å¦‚æœå¯ç”¨çˆ¶å­chunkæ¨¡å¼ï¼š
        - çˆ¶ chunk å†™å…¥ SQLiteï¼ˆparent_storeï¼‰
        - å­ chunk å†™å…¥ Chromaï¼ˆå‘é‡æ£€ç´¢ï¼‰
        """
        if settings.ENABLE_PARENT_CHILD:
            self._add_documents_parent_child(documents)
        else:
            # æ™®é€šæ¨¡å¼ï¼šä½¿ç”¨å¸¦é‡è¯•çš„æ‰¹é‡æ·»åŠ 
            try:
                self._add_documents_with_retry(documents, [str(uuid.uuid4()) for _ in documents])
            except Exception:
                # å¦‚æœæ‰¹é‡æ·»åŠ å¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥æ·»åŠ ï¼ˆå…¼å®¹æ€§ï¼‰
                print("  âš  æ‰¹é‡æ·»åŠ å¤±è´¥ï¼Œå°è¯•ç›´æ¥æ·»åŠ ...")
            self.vector_store.add_documents(documents)

    def query(self, query_str: str, top_k: int = 5) -> List[Document]:
        """
        æŸ¥è¯¢ç›¸ä¼¼æ–‡æ¡£
        
        å¦‚æœå¯ç”¨çˆ¶å­chunkæ¨¡å¼ï¼Œä¼˜å…ˆè¿”å›çˆ¶chunk
        """
        if settings.ENABLE_PARENT_CHILD:
            # åªæ£€ç´¢å­ chunkï¼Œç„¶åå›æº¯çˆ¶ chunkï¼ˆçˆ¶ chunk ä¸å…¥å‘é‡åº“ï¼‰
            results = self.vector_store.similarity_search_with_score(
                query_str, k=top_k * 4
            )

            # parent_id -> best_score / matched_children
            parent_best: Dict[str, float] = {}
            parent_children: Dict[str, List[Document]] = {}

            for doc, score in results:
                parent_id = (doc.metadata or {}).get("parent_id")
                if not parent_id:
                    continue
                if parent_id not in parent_best or score > parent_best[parent_id]:
                    parent_best[parent_id] = score
                parent_children.setdefault(parent_id, []).append(doc)

            # æŒ‰ best_score å– top_k ä¸ª parent
            sorted_parent_ids = sorted(
                parent_best.keys(), key=lambda pid: parent_best[pid], reverse=True
            )[:top_k]

            parent_docs: List[Document] = []
            for pid in sorted_parent_ids:
                parent = self.parent_store.get_parent(pid)
                if not parent:
                    continue
                md = dict(parent.get("metadata") or {})
                matched = parent_children.get(pid, [])[:3]
                md["matched_children_preview"] = [
                    (c.page_content or "")[:200] for c in matched
                ]
                parent_text = self.assemble_parent_text_from_children(pid)
                parent_docs.append(
                    Document(page_content=parent_text, metadata=md)
                )
            return parent_docs
        else:
            # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢
            results = self.vector_store.similarity_search_with_score(
                query_str, k=top_k
            )
            # æ™®é€šæ¨¡å¼ï¼šç›´æ¥è¿”å›ç»“æœ
            return [doc for doc, _ in results[:top_k]]

    def delete_all(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"""
        # åˆ é™¤é›†åˆå¹¶é‡æ–°åˆ›å»º
        self.vector_store.delete_collection()
        self.vector_store = Chroma(
            persist_directory=settings.CHROMA_PERSIST_DIR,
            collection_name=settings.CHROMA_COLLECTION_NAME,
            embedding_function=self.embedding_model,
        )
        # åŒæ—¶æ¸…ç©ºçˆ¶ chunk å­˜å‚¨
        try:
            self.parent_store.clear()
        except Exception:
            pass
        return True

    def delete_chunk_by_id(self, chunk_id: str) -> bool:
        """åˆ é™¤å•ä¸ª chunkï¼ˆæŒ‰ idï¼‰"""
        # Chroma collection.delete æ”¯æŒ ids å‚æ•°
        self.vector_store._collection.delete(ids=[chunk_id])
        return True

    def delete_parent_by_id(self, parent_id: str) -> bool:
        """åˆ é™¤çˆ¶ chunkï¼šåŒæ—¶åˆ é™¤è¯¥çˆ¶ä¸‹æ‰€æœ‰å­ chunk + SQLite è®°å½•"""
        # åˆ é™¤å­ chunkï¼ˆwhereï¼‰
        try:
            self.vector_store._collection.delete(where={"parent_id": parent_id})
        except Exception:
            # å…¼å®¹æ€§å…œåº•ï¼šå¦‚æœ where delete ä¸æ”¯æŒï¼Œé€€åŒ–ä¸º get åæŒ‰ ids delete
            results = self.vector_store._collection.get(
                where={"parent_id": parent_id},
                include=[],
            )
            ids = results.get("ids", []) or []
            if ids:
                self.vector_store._collection.delete(ids=ids)
        # åˆ é™¤çˆ¶ chunk è®°å½•
        self.parent_store.delete_parent(parent_id)
        return True

    def delete_parents_by_file(self, file_name: str) -> bool:
        """åˆ é™¤æŸä¸ªæ–‡ä»¶å¯¹åº”çš„çˆ¶å’Œå­ chunk"""
        if not file_name:
            return False
        parent_ids = self.parent_store.delete_parents_by_file_name(file_name)
        if parent_ids:
            try:
                self.vector_store._collection.delete(where={"parent_id": {"$in": parent_ids}})
            except Exception:
                # å…¼å®¹è€ç‰ˆæœ¬ï¼šé€ä¸ªåˆ é™¤
                for pid in parent_ids:
                    self.vector_store._collection.delete(where={"parent_id": pid})
        return True

    def file_exists(self, file_name: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»åœ¨çŸ¥è¯†åº“ä¸­"""
        if not settings.ENABLE_PARENT_CHILD:
            # éçˆ¶å­æ¨¡å¼ï¼šæ£€æŸ¥ ChromaDB ä¸­æ˜¯å¦æœ‰è¯¥æ–‡ä»¶çš„æ–‡æ¡£
            try:
                results = self.vector_store._collection.get(
                    where={"file_name": file_name},
                    limit=1,
                )
                return len(results.get("ids", [])) > 0
            except Exception:
                return False
        else:
            # çˆ¶å­æ¨¡å¼ï¼šæ£€æŸ¥ SQLite ä¸­æ˜¯å¦æœ‰è¯¥æ–‡ä»¶çš„çˆ¶ chunk
            parent_ids = self.parent_store.list_parent_ids_by_file(file_name)
            return len(parent_ids) > 0

    def assemble_parent_text_from_children(self, parent_id: str) -> str:
        """æ ¹æ®å­ chunk æ‹¼æ¥çˆ¶ chunk çš„æ–‡æœ¬å†…å®¹"""
        children = self.list_children_by_parent_id(parent_id)
        texts = []
        for child in children:
            text = (child.get("text") or "").strip()
            if text:
                texts.append(text)
        if texts:
            return "\n".join(texts)
        parent = self.parent_store.get_parent(parent_id)
        if parent:
            return parent.get("text") or ""
        return ""

    def update_child_chunk_text(self, chunk_id: str, new_text: str) -> bool:
        """æ›´æ–°æŒ‡å®šå­ chunk çš„æ–‡æœ¬å†…å®¹"""
        if not settings.ENABLE_PARENT_CHILD:
            return False
        # è·å–å½“å‰ chunk metadata
        try:
            existing = self.vector_store._collection.get(
                ids=[chunk_id], include=["documents", "metadatas"]
            )
        except Exception:
            return False
        ids = existing.get("ids") or []
        if not ids:
            return False
        metadatas = existing.get("metadatas") or [{}]
        metadata = metadatas[0] if metadatas else {}
        metadata = metadata or {}
        doc = Document(page_content=new_text or "", metadata=metadata)
        self.vector_store.add_documents([doc], ids=[chunk_id])
        return True

    def add_child_chunk(self, parent_id: str, text: str) -> Optional[str]:
        """ä¸ºæŒ‡å®šçˆ¶ chunk æ·»åŠ ä¸€ä¸ªæ–°çš„å­ chunk"""
        if not settings.ENABLE_PARENT_CHILD:
            return None

        parent = self.parent_store.get_parent(parent_id)
        if not parent:
            return None

        children = self.list_children_by_parent_id(parent_id)
        max_index = -1
        for child in children:
            idx = child.get("child_index")
            if isinstance(idx, int):
                max_index = max(max_index, idx)
            else:
                try:
                    max_index = max(max_index, int(idx))
                except (TypeError, ValueError):
                    continue

        new_index = max_index + 1

        metadata: Dict[str, Any] = dict(parent.get("metadata") or {})
        metadata.update(
            {
                "chunk_type": "child",
                "split_strategy": document_loader.split_strategy,
                "parent_id": parent_id,
                "parent_index": metadata.get("parent_index"),
                "child_index": new_index,
            }
        )

        child_id = f"{parent_id}:{new_index}"
        child_doc = Document(page_content=text or "", metadata=metadata)
        self.vector_store.add_documents([child_doc], ids=[child_id])
        return child_id

    def clear_collection(self) -> bool:
        """æ¸…ç©ºæ•´ä¸ª collectionï¼ˆç­‰ä»·äº delete_allï¼‰"""
        return bool(self.delete_all())

    def get_retriever(self, top_k: int = 5):
        """è·å–æ£€ç´¢å™¨"""
        return self.vector_store.as_retriever(
            search_kwargs={"k": top_k}
        )

    # ==================== WebAPI æ€§è´¨çš„å‡½æ•°ï¼ˆè¿”å›å­—å…¸ï¼Œç”¨äºAPIå“åº”ï¼‰ ====================

    def get_collection_info(self) -> dict:
        """è·å–é›†åˆä¿¡æ¯"""
        count = self.vector_store._collection.count()
        info = {
            "collection_name": settings.CHROMA_COLLECTION_NAME,
            "document_count": count,
            "persist_dir": settings.CHROMA_PERSIST_DIR,
        }
        if settings.ENABLE_PARENT_CHILD:
            try:
                info["parent_count"] = self.parent_store.count()
            except Exception:
                info["parent_count"] = None
        return info

    def get_all_documents(self) -> List[dict]:
        """
        è·å–æ‰€æœ‰æ–‡æ¡£å—
        
        Returns:
            æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«æ–‡æœ¬å’Œå…ƒæ•°æ®
        """
        # ä»ChromaDBè·å–æ‰€æœ‰æ•°æ®ï¼ˆçˆ¶å­æ¨¡å¼ä¸‹åªå­˜å­ chunkï¼‰
        results = self.vector_store._collection.get()
        
        documents = []
        if results and "ids" in results:
            ids = results.get("ids", [])
            documents_data = results.get("documents", [])
            metadatas = results.get("metadatas", [])
            
            for i, doc_id in enumerate(ids):
                doc_text = documents_data[i] if i < len(documents_data) else ""
                doc_metadata = metadatas[i] if i < len(metadatas) else {}
                
                documents.append(
                    {"id": doc_id, "text": doc_text, "metadata": doc_metadata}
                )
        
        return documents

    def list_chunks_page(
        self,
        limit: int = 20,
        offset: int = 0,
        where: Optional[Dict] = None,
        where_document: Optional[Dict] = None,
    ) -> Dict[str, any]:
        """
        åˆ†é¡µè·å– chunk åˆ—è¡¨ï¼ˆå°½é‡èµ° Chroma åŸç”Ÿè¿‡æ»¤ï¼‰ã€‚

        Args:
            limit: æ¯é¡µæ¡æ•°
            offset: åç§»é‡
            where: metadata è¿‡æ»¤æ¡ä»¶ï¼ˆChroma whereï¼‰
            where_document: æ–‡æœ¬è¿‡æ»¤æ¡ä»¶ï¼ˆChroma where_documentï¼Œä¾‹å¦‚ {"$contains": "foo"}ï¼‰
        """
        include = ["documents", "metadatas"]
        # Chroma collection.get æ”¯æŒ limit/offset/where/where_documentï¼ˆä¸åŒç‰ˆæœ¬å…¼å®¹æ€§ç•¥æœ‰å·®å¼‚ï¼‰
        try:
            results = self.vector_store._collection.get(
                include=include,
                limit=limit,
                offset=offset,
                where=where,
                where_document=where_document,
            )
        except TypeError:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šæ²¡æœ‰ where_document æˆ– offset å‚æ•°
            results = self.vector_store._collection.get(
                include=include,
                where=where,
            )
            # æ‰‹åŠ¨åˆ†é¡µï¼ˆé€€åŒ–æ–¹æ¡ˆï¼‰
            ids = results.get("ids", []) or []
            documents_data = results.get("documents", []) or []
            metadatas = results.get("metadatas", []) or []
            ids = ids[offset : offset + limit]
            documents_data = documents_data[offset : offset + limit]
            metadatas = metadatas[offset : offset + limit]
            results = {"ids": ids, "documents": documents_data, "metadatas": metadatas}

        ids = results.get("ids", []) or []
        documents_data = results.get("documents", []) or []
        metadatas = results.get("metadatas", []) or []

        chunks = []
        for i, doc_id in enumerate(ids):
            chunks.append(
                {
                    "id": doc_id,
                    "text": documents_data[i] if i < len(documents_data) else "",
                    "metadata": metadatas[i] if i < len(metadatas) else {},
                }
            )

        # total: ä»…åœ¨æ— è¿‡æ»¤æ—¶å¯é ï¼ˆcount ä¸æ”¯æŒ whereï¼‰ï¼›è¿‡æ»¤æ—¶ç”¨ None è¡¨ç¤ºæœªçŸ¥
        total = None
        if not where and not where_document:
            try:
                total = self.vector_store._collection.count()
            except Exception:
                total = None

        return {"chunks": chunks, "total": total}

    def get_chunk_by_id(self, chunk_id: str) -> Dict[str, any]:
        """æ ¹æ® chunk id è·å–è¯¦æƒ…"""
        results = self.vector_store._collection.get(
            ids=[chunk_id],
            include=["documents", "metadatas"],
        )
        ids = results.get("ids", []) or []
        if not ids:
            return {"id": chunk_id, "text": "", "metadata": {}, "found": False}
        doc = (results.get("documents") or [""])[0]
        md = (results.get("metadatas") or [{}])[0]
        return {"id": ids[0], "text": doc or "", "metadata": md or {}, "found": True}

    def list_parents_page(
        self,
        limit: int = 20,
        offset: int = 0,
        file_name: str = "",
        q: str = "",
    ) -> Dict[str, Any]:
        """åˆ†é¡µåˆ—å‡ºçˆ¶ chunkï¼ˆSQLiteï¼‰"""
        items, total = self.parent_store.list_parents(
            limit=limit, offset=offset, file_name=file_name, q=q
        )
        for item in items:
            item["assembled_text"] = self.assemble_parent_text_from_children(
                item["parent_id"]
            )
        return {"parents": items, "total": total}

    def list_parent_files(self) -> List[Dict[str, Any]]:
        """è¿”å›æŒ‰æ–‡ä»¶èšåˆçš„çˆ¶ chunk æ±‡æ€»ä¿¡æ¯"""
        if not settings.ENABLE_PARENT_CHILD:
            return []
        return self.parent_store.list_file_groups()

    def list_parent_files_page(
        self,
        limit: int = 20,
        offset: int = 0,
        search: str = "",
        sort_by: str = "name",
        sort_order: str = "asc",
    ) -> Dict[str, Any]:
        """è¿”å›æŒ‰æ–‡ä»¶èšåˆçš„çˆ¶ chunk æ±‡æ€»ä¿¡æ¯ï¼ˆåˆ†é¡µç‰ˆæœ¬ï¼Œæ”¯æŒæœç´¢å’Œæ’åºï¼‰"""
        if not settings.ENABLE_PARENT_CHILD:
            return {"files": [], "total": 0}
        files, total = self.parent_store.list_file_groups_page(
            limit=limit,
            offset=offset,
            search=search,
            sort_by=sort_by,
            sort_order=sort_order,
        )
        return {"files": files, "total": total}

    def get_parent_by_id(self, parent_id: str) -> Dict[str, Any]:
        """è·å–çˆ¶ chunkï¼ˆSQLiteï¼‰"""
        parent = self.parent_store.get_parent(parent_id)
        if not parent:
            return {"parent_id": parent_id, "text": "", "metadata": {}, "found": False}
        return {**parent, "found": True}

    def list_children_by_parent_id(self, parent_id: str) -> List[Dict[str, Any]]:
        """è·å–æŸä¸ªçˆ¶ chunk çš„æ‰€æœ‰å­ chunkï¼ˆä» Chroma where è¿‡æ»¤ï¼‰"""
        results = self.vector_store._collection.get(
            where={"parent_id": parent_id},
            include=["documents", "metadatas"],
        )
        ids = results.get("ids", []) or []
        documents_data = results.get("documents", []) or []
        metadatas = results.get("metadatas", []) or []

        children: List[Dict[str, Any]] = []
        for i, doc_id in enumerate(ids):
            md = metadatas[i] if i < len(metadatas) else {}
            children.append(
                {
                    "id": doc_id,
                    "text": documents_data[i] if i < len(documents_data) else "",
                    "metadata": md,
                    "child_index": (md or {}).get("child_index", i),
                }
            )
        children.sort(key=lambda x: (x.get("child_index") is None, x.get("child_index", 0)))
        return children


# å…¨å±€å‘é‡å­˜å‚¨ç®¡ç†å™¨å®ä¾‹
vector_store_manager = VectorStoreManager()
